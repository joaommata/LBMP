{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5c9be8",
   "metadata": {},
   "source": [
    "### Testing the transcribing of the audio file using Whisper models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e7fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c582d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model_base = whisper.load_model(\u001b[33m\"\u001b[39m\u001b[33mbase\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# options: tiny, base, small, medium, large\u001b[39;00m\n\u001b[32m      3\u001b[39m model_medium = whisper.load_model(\u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# options: tiny, base, small, medium, large\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_turbo = \u001b[43mwhisper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mturbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# options: tiny, base, small, medium, large\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/__init__.py:150\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, device, download_root, in_memory)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    144\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found; available models = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_models()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    145\u001b[39m     )\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    148\u001b[39m     io.BytesIO(checkpoint_file) \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    149\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[32m    153\u001b[39m dims = ModelDimensions(**checkpoint[\u001b[33m\"\u001b[39m\u001b[33mdims\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/serialization.py:1516\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1523\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1524\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/serialization.py:2114\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2112\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2113\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2114\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2115\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2117\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/_weights_only_unpickler.py:532\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    525\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ):\n\u001b[32m    529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    530\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    531\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    534\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/serialization.py:2078\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2076\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2077\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2078\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/serialization.py:2031\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2024\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_offset != zip_file.get_record_offset(name):\n\u001b[32m   2025\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2026\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThis is a debug assert that was run as the `TORCH_SERIALIZATION_DEBUG` environment \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2027\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvariable was set: Incorrect offset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m expected \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2028\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file.get_record_offset(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2029\u001b[39m             )\n\u001b[32m   2030\u001b[39m     storage = (\n\u001b[32m-> \u001b[39m\u001b[32m2031\u001b[39m         \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2032\u001b[39m         ._typed_storage()\n\u001b[32m   2033\u001b[39m         ._untyped_storage\n\u001b[32m   2034\u001b[39m     )\n\u001b[32m   2035\u001b[39m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[32m   2036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_base = whisper.load_model(\"base\")  # options: tiny, base, small, medium, large\n",
    "model_medium = whisper.load_model(\"medium\")  # options: tiny, base, small, medium, large\n",
    "model_turbo = whisper.load_model(\"turbo\")  # options: tiny, base, small, medium, large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaomata/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "result_base= model_base.transcribe(\"/Users/joaomata/Desktop/LBMP/E-DAIC/600_P/600_AUDIO.wav\")\n",
    "with open(\"result_base_600.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result_base, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a068ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [01:03<00:00, 24.0MiB/s]\n",
      "/Users/joaomata/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Python(97011) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "result_medium = model_large.transcribe(\"/Users/joaomata/Desktop/LBMP/E-DAIC/600_P/600_AUDIO.wav\")\n",
    "with open(\"result_medium.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result_medium, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce3b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaomata/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Python(97610) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "result_turbo = model_turbo.transcribe(\"/Users/joaomata/Desktop/LBMP/E-DAIC/600_P/600_AUDIO.wav\")\n",
    "with open(\"result_turbo.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result_turbo, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3855021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the transcription\n",
    "print(\"Result (Base)\")  \n",
    "print(result_base[\"text\"])\n",
    "print(\"Result (Large)\")\n",
    "print(result_medium[\"text\"])\n",
    "print(\"Result (turbo)\")\n",
    "print(result_turbo[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20225230",
   "metadata": {},
   "source": [
    "### Using DepRoBERTa to classify transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58bb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaomata/Desktop/LBMP/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rafalposwiata/deproberta-large-depression\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rafalposwiata/deproberta-large-depression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa681fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Okay. So you're in the bridge, you're in the hoppy, up here for just a second, and I'll go ahead and shrink her back down. Okay. So there she is. So I'm going to go ahead and shrink her back down while I continue setting some things up. Okay. Come on. Okay. Okay. Are you okay with this? Yes. Okay. So, are you going to do it today? I'm fine. Let's go. Where are you from originally? Let's go. Really? Yes. Where are you from originally? Where are you from originally? The weather. Where are you from originally? The crime. I see what you mean. Do you consider yourself a man? I don't know, Demise. Sometimes when I'm feeling tense, I turn on the fist and scream saber. Hey, I know it's not polite, but it's the best I've got. What do you do to relax? Rhyp poetry. What are some things that make you really not? What happens to your mind? Nothing. What do you do when you're in a lot of it? Listen to music. How are you going to control your tongue? Very good. When was the last time you heard this? About a week ago, my husband over cigarettes. Because he wants to stop it now, though. What did you say at school? I dropped out when I was in the Length grade. I could say it. What do you do now? I'm unemployed. Nice. What's your dream job? I would like to become a famous poetry writer. Do you have a job? No. Why? It's too expensive. What's your most memorable experiences? I always let my friends and friends have fun. That's good. I close our into your family. I'm very close. There's something that's been a positive influence in your life. My husband. Can you tell me about that? I used to be a drug addict and he helped me overcome my addiction. I feel better. It's fine. Do you have a mate? No, just my husband. Have you ever served in military? No. Have you ever been diagnosed with PTSD? Yes. How long have you been diagnosed with depression? 2002. Did you think you had an album before you found out? Not really. I overdosed on some pills. 2002. Do you still go with therapy now? No. Excuse me? I'm not a tenant therapy right now. Can you tell me about that? I'm not a tenant therapy right now. I don't go. I'm not comfortable talking about this. I don't feel comfortable talking about this. How do you see me there? I'm not comfortable talking about this. Okay. Sad all the time. I want to do anything. Sleep too much. I don't know. How do you help with that? I run poetry. It helps me a lot. Well, when I get inspired, I just start writing what's on my mind. I'm not comfortable talking about this. I'm not comfortable talking about this. I'm not comfortable talking about this. I'm not comfortable talking about this. Do you feel that way often? Yes. Time to go down? No. How easy is it to get a good sleep? I just go lay down and just do some music for a little while and you know fall asleep? What are you going to do when you just sleep? Cranky. One of the last time I was really happy. Today. I don't know. How are your best friends writing? A happy person. How going? Nothing for high school. I think I lost everything I need to. Thanks for sharing your thoughts with me. Goodbye. I'm just happy today. I feel like...\n",
      "Logits: tensor([[ 0.0612,  0.4660, -0.0235]])\n",
      "Probabilities: tensor([[0.2926, 0.4386, 0.2688]])\n",
      "Predicted class: 1\n",
      "Prediction: Moderate Depression\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load texts from pickle files\n",
    "with open(\"result_base_600.pkl\", \"rb\") as f:\n",
    "    result_base = pickle.load(f)\n",
    "with open(\"result_medium.pkl\", \"rb\") as f:\n",
    "    result_medium = pickle.load(f)\n",
    "with open(\"result_turbo.pkl\", \"rb\") as f:\n",
    "    result_turbo = pickle.load(f)\n",
    "\n",
    "texts = {\n",
    "    \"Base\": result_base[\"text\"],\n",
    "    \"Medium\": result_medium[\"text\"],\n",
    "    \"Turbo\": result_turbo[\"text\"]\n",
    "}\n",
    "\n",
    "labels = [\"No Depression\", \"Moderate Depression\", \"Depression\"]\n",
    "\n",
    "for model_name, text in texts.items():\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_idx = torch.argmax(probs).item()\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Text: {text}\") \n",
    "    print(f\"Logits: {logits}\")\n",
    "    print(f\"Probabilities: {probs}\")\n",
    "    print(f\"Predicted class: {pred_idx} ({labels[pred_idx]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
