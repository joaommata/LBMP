{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import whisper\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def load_whisper_models():\n",
    "    \"\"\"Load Whisper models once for reuse.\"\"\"\n",
    "    #model_base = whisper.load_model(\"base\")\n",
    "    model_medium = whisper.load_model(\"medium\")\n",
    "    #model_turbo = whisper.load_model(\"turbo\")\n",
    "    return model_medium\n",
    "\n",
    "\n",
    "def transcribe_audio(model, audio_path):\n",
    "    \"\"\"Transcribe audio file using a Whisper model.\"\"\"\n",
    "    return model.transcribe(audio_path)\n",
    "\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def setup_google_llm(api_key: str, model_name: str, temperature: float = 0.7):\n",
    "    \"\"\"Setup Google Gemini LLM for processing.\"\"\"\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCAQ7iYFHHlLi9pqKku_j_elEp9hOVO5Ng\" \n",
    "    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature)\n",
    "\n",
    "\n",
    "def clean_transcript_with_gemini(llm, transcript: str) -> str:\n",
    "    \"\"\"Use Gemini to clean a transcript with the specified prompt.\"\"\"\n",
    "    text_clean_up_prompt = \"\"\" Context:\n",
    "This transcript comes from an interview conversation, automatically transcribed by Whisper. Due to occasional low audio quality, some questions may be missing answers, and some phrases may be duplicated multiple times. Our goal is to clean this transcript automatically for better downstream analysis by removing unanswered questions and handling duplications without manual edits.\n",
    "\n",
    "Prompt:\n",
    "\"This transcript is from a conversation between an interviewer and an interviewee. Please refine the transcript by doing the following:\n",
    "\n",
    "Remove any questions that do not have an answer following them. Note that answers can be brief, including simple ‘yes’ or ‘no’ responses.\n",
    "Retain only the most relevant and concise form of repeated or duplicated phrases—remove unnecessary repetitions while preserving meaning.\n",
    "Do not add new content or interpret responses, only clean and clarify the existing text for easier analysis.\"\n",
    "\n",
    "Here is the transcript to clean:\n",
    "{transcript}\n",
    "\"\"\"\n",
    "    prompt = text_clean_up_prompt.format(transcript=transcript)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "\n",
    "def summarize_transcript_with_gemini(llm, transcript: str) -> str:\n",
    "    \"\"\"Use Gemini to summarize the interview transcript in first person.\"\"\"\n",
    "    summary_prompt = \"\"\"\n",
    "Task:\n",
    "You will read the following interview transcript. \n",
    "Your goal is to summarize the key points that relate to the interviewee’s experiences with depression. \n",
    "Please be concise and write the summary from the first-person perspective, as if you are the interviewee sharing your own story.\n",
    "\n",
    "Here is the transcript:\n",
    "{transcript}\n",
    "\"\"\"\n",
    "    prompt = summary_prompt.format(transcript=transcript)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_process_patient_archives(\n",
    "    data_dir, train_csv_path, llm, whisper_model, patients_to_process=None\n",
    "):\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    train_ids = set(train_df['Participant_ID'].astype(str))\n",
    "    \n",
    "    # Create a dict for quick PHQ_Score lookup by patient_id (as string)\n",
    "    phq_score_map = train_df.set_index(train_df['Participant_ID'].astype(str))['PHQ_Score'].to_dict()\n",
    "    \n",
    "    data = []\n",
    "    archives = [f for f in os.listdir(data_dir) if f.endswith(\".tar.gz\")]\n",
    "    \n",
    "    # If patients_to_process is provided, filter train_ids accordingly\n",
    "    if patients_to_process is not None:\n",
    "        # ensure strings\n",
    "        patients_to_process = set(str(p) for p in patients_to_process)\n",
    "        # intersect with train_ids to keep only valid IDs\n",
    "        valid_patients = train_ids.intersection(patients_to_process)\n",
    "    else:\n",
    "        valid_patients = train_ids\n",
    "    \n",
    "    for archive_name in tqdm(archives, desc=\"Patients processed\"):\n",
    "        patient_id = archive_name.split(\"_\")[0]\n",
    "        \n",
    "        if patient_id in valid_patients:\n",
    "            archive_path = os.path.join(data_dir, archive_name)\n",
    "            print(f\"Processing patient {patient_id}\")\n",
    "            \n",
    "            with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(path=tmpdir)\n",
    "                \n",
    "                # Process all audio files inside\n",
    "                for root, _, files in os.walk(tmpdir):\n",
    "                    for file in files:\n",
    "                        if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n",
    "                            audio_path = os.path.join(root, file)\n",
    "                            raw_transcript = transcribe_audio(whisper_model, audio_path)[\"text\"]\n",
    "                            print(\"Audio transcribed for patient:\", patient_id)\n",
    "                            cleaned_transcript = clean_transcript_with_gemini(llm, raw_transcript)\n",
    "                            print(\"Transcript cleaned\")\n",
    "                            summarized_transcript = summarize_transcript_with_gemini(llm, cleaned_transcript)\n",
    "                            print(\"Transcript summarized\")\n",
    "                            \n",
    "                            data.append({\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"transcript\": cleaned_transcript,\n",
    "                                \"summary\": summarized_transcript,\n",
    "                                \"PHQ_Score\": phq_score_map.get(patient_id)\n",
    "                            })\n",
    "        else:\n",
    "            # Skipping patients not in the selected subset or train split\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['300_P.tar.gz', '301_P.tar.gz', '302_P.tar.gz', '303_P.tar.gz', '304_P.tar.gz', '305_P.tar.gz', '306_P.tar.gz', '307_P.tar.gz', '308_P.tar.gz', '309_P.tar.gz', '310_P.tar.gz', '311_P.tar.gz', '312_P.tar.gz', '313_P.tar.gz', '314_P.tar.gz', '315_P.tar.gz', '316_P.tar.gz', '317_P.tar.gz', '318_P.tar.gz', '319_P.tar.gz', '320_P.tar.gz', '321_P.tar.gz', '322_P.tar.gz', '323_P.tar.gz', '324_P.tar.gz', '325_P.tar.gz', '326_P.tar.gz', '327_P.tar.gz', '328_P.tar.gz', '329_P.tar.gz', '330_P.tar.gz', '331_P.tar.gz', '332_P.tar.gz', '333_P.tar.gz', '334_P.tar.gz', '335_P.tar.gz', '336_P.tar.gz', '337_P.tar.gz', '338_P.tar.gz', '339_P.tar.gz', '340_P.tar.gz', '341_P.tar.gz', '343_P.tar.gz', '344_P.tar.gz', '345_P.tar.gz', '346_P.tar.gz', '347_P.tar.gz', '348_P.tar.gz', '349_P.tar.gz', '350_P.tar.gz', '351_P.tar.gz', '352_P.tar.gz', '353_P.tar.gz', '354_P.tar.gz', '355_P.tar.gz', '356_P.tar.gz', '357_P.tar.gz', '358_P.tar.gz', '359_P.tar.gz', '360_P.tar.gz', '361_P.tar.gz', '362_P.tar.gz', '363_P.tar.gz', '364_P.tar.gz', '365_P.tar.gz', '366_P.tar.gz', '367_P.tar.gz', '368_P.tar.gz', '369_P.tar.gz', '370_P.tar.gz', '371_P.tar.gz', '372_P.tar.gz', '373_P.tar.gz', '374_P.tar.gz', '375_P.tar.gz', '376_P.tar.gz', '377_P.tar.gz', '378_P.tar.gz', '379_P.tar.gz', '380_P.tar.gz', '381_P.tar.gz', '382_P.tar.gz', '383_P.tar.gz', '384_P.tar.gz', '385_P.tar.gz', '386_P.tar.gz', '387_P.tar.gz', '388_P.tar.gz', '389_P.tar.gz', '390_P.tar.gz', '391_P.tar.gz', '392_P.tar.gz', '393_P.tar.gz', '395_P.tar.gz', '396_P.tar.gz', '397_P.tar.gz', '399_P.tar.gz', '400_P.tar.gz', '401_P.tar.gz', '402_P.tar.gz', '403_P.tar.gz', '404_P.tar.gz', '405_P.tar.gz', '406_P.tar.gz', '407_P.tar.gz', '408_P.tar.gz', '409_P.tar.gz', '410_P.tar.gz', '411_P.tar.gz', '412_P.tar.gz', '413_P.tar.gz', '414_P.tar.gz', '415_P.tar.gz', '416_P.tar.gz', '417_P.tar.gz', '418_P.tar.gz', '419_P.tar.gz', '420_P.tar.gz', '421_P.tar.gz', '422_P.tar.gz', '423_P.tar.gz', '424_P.tar.gz', '425_P.tar.gz', '426_P.tar.gz', '427_P.tar.gz', '428_P.tar.gz', '429_P.tar.gz', '430_P.tar.gz', '431_P.tar.gz', '432_P.tar.gz', '433_P.tar.gz', '434_P.tar.gz', '435_P.tar.gz', '436_P.tar.gz', '437_P.tar.gz', '438_P.tar.gz', '439_P.tar.gz', '440_P.tar.gz', '441_P.tar.gz', '442_P.tar.gz', '443_P.tar.gz', '444_P.tar.gz', '445_P.tar.gz', '446_P.tar.gz', '447_P.tar.gz', '448_P.tar.gz', '449_P.tar.gz', '450_P.tar.gz', '451_P.tar.gz', '452_P.tar.gz', '453_P.tar.gz', '454_P.tar.gz', '455_P.tar.gz', '456_P.tar.gz', '457_P.tar.gz', '458_P.tar.gz', '459_P.tar.gz', '461_P.tar.gz', '462_P.tar.gz', '463_P.tar.gz', '464_P.tar.gz', '465_P.tar.gz', '466_P.tar.gz', '467_P.tar.gz', '468_P.tar.gz', '469_P.tar.gz', '470_P.tar.gz', '471_P.tar.gz', '472_P.tar.gz', '473_P.tar.gz', '474_P.tar.gz', '475_P.tar.gz', '476_P.tar.gz', '477_P.tar.gz', '478_P.tar.gz', '479_P.tar.gz', '480_P.tar.gz', '481_P.tar.gz', '482_P.tar.gz', '483_P.tar.gz', '484_P.tar.gz', '485_P.tar.gz', '486_P.tar.gz', '487_P.tar.gz', '488_P.tar.gz', '489_P.tar.gz', '490_P.tar.gz', '491_P.tar.gz', '492_P.tar.gz', '600_P.tar.gz', '601_P.tar.gz', '602_P.tar.gz', '603_P.tar.gz', '604_P.tar.gz', '605_P.tar.gz', '606_P.tar.gz', '607_P.tar.gz', '608_P.tar.gz', '609_P.tar.gz', '612_P.tar.gz', '615_P.tar.gz', '617_P.tar.gz', '618_P.tar.gz', '619_P.tar.gz', '620_P.tar.gz', '622_P.tar.gz', '623_P.tar.gz', '624_P.tar.gz', '625_P.tar.gz', '626_P.tar.gz', '627_P.tar.gz', '628_P.tar.gz', '629_P.tar.gz', '631_P.tar.gz', '632_P.tar.gz', '633_P.tar.gz', '634_P.tar.gz', '635_P.tar.gz', '636_P.tar.gz', '637_P.tar.gz', '638_P.tar.gz', '640_P.tar.gz', '641_P.tar.gz', '649_P.tar.gz', '650_P.tar.gz', '651_P.tar.gz', '652_P.tar.gz', '653_P.tar.gz', '654_P.tar.gz', '655_P.tar.gz', '656_P.tar.gz', '657_P.tar.gz', '658_P.tar.gz', '659_P.tar.gz', '660_P.tar.gz', '661_P.tar.gz', '662_P.tar.gz', '663_P.tar.gz', '664_P.tar.gz', '666_P.tar.gz', '667_P.tar.gz', '669_P.tar.gz', '670_P.tar.gz', '673_P.tar.gz', '676_P.tar.gz', '677_P.tar.gz', '679_P.tar.gz', '680_P.tar.gz', '682_P.tar.gz', '683_P.tar.gz', '684_P.tar.gz', '687_P.tar.gz', '688_P.tar.gz', '689_P.tar.gz', '691_P.tar.gz', '692_P.tar.gz', '693_P.tar.gz', '695_P.tar.gz', '696_P.tar.gz', '697_P.tar.gz', '698_P.tar.gz', '699_P.tar.gz', '702_P.tar.gz', '703_P.tar.gz', '705_P.tar.gz', '707_P.tar.gz', '708_P.tar.gz', '709_P.tar.gz', '710_P.tar.gz', '712_P.tar.gz', '713_P.tar.gz', '715_P.tar.gz', '716_P.tar.gz', '717_P.tar.gz', '718_P.tar.gz']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"/Volumes/My Passport for Mac/Extended DAIC-WOZ Database/data\"\n",
    "print(os.listdir(data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper models loaded.\n",
      "LLM loaded.\n",
      "Random 50 patients selected: ['438', '345', '356', '362', '325', '363', '404', '474', '446', '305', '361', '452', '337', '386', '426', '335', '459', '311', '469', '367', '463', '464', '315', '351', '420', '383', '309', '304', '421', '416', '333', '358', '418', '355', '323', '405', '390', '353', '378', '428', '389', '330', '472', '458', '360', '407', '395', '419', '303', '354']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patients processed:   0%|          | 0/275 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient 303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaomata/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Patients processed:   1%|          | 3/275 [00:48<1:13:30, 16.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRandom 50 patients selected:\u001b[39m\u001b[33m\"\u001b[39m, random_50_patients)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Run the processing to create new transctripts and summaries for eachh patient\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m transcripts_df = \u001b[43mextract_and_process_patient_archives\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatients_to_process\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_50_patients\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m transcripts_df.to_csv(\u001b[33m\"\u001b[39m\u001b[33mtrain_patient_transcripts_50_cleaned.tsv\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mextract_and_process_patient_archives\u001b[39m\u001b[34m(data_dir, train_csv_path, llm, whisper_model, patients_to_process)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.wav\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.mp3\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     44\u001b[39m     audio_path = os.path.join(root, file)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     raw_transcript = \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhisper_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAudio transcribed for patient:\u001b[39m\u001b[33m\"\u001b[39m, patient_id)\n\u001b[32m     47\u001b[39m     cleaned_transcript = clean_transcript_with_gemini(llm, raw_transcript)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtranscribe_audio\u001b[39m\u001b[34m(model, audio_path)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranscribe_audio\u001b[39m(model, audio_path):\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transcribe audio file using a Whisper model.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/transcribe.py:279\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    276\u001b[39m mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n\u001b[32m    278\u001b[39m decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    283\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/transcribe.py:195\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    192\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    194\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    199\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    200\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    201\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/decoding.py:737\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    734\u001b[39m tokens = tokens.repeat_interleave(\u001b[38;5;28mself\u001b[39m.n_group, dim=\u001b[32m0\u001b[39m).to(audio_features.device)\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m tokens, sum_logprobs, no_speech_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[32m    740\u001b[39m audio_features = audio_features[:: \u001b[38;5;28mself\u001b[39m.n_group]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/decoding.py:687\u001b[39m, in \u001b[36mDecodingTask._main_loop\u001b[39m\u001b[34m(self, audio_features, tokens)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.sample_len):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    690\u001b[39m             i == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.no_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    691\u001b[39m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[32m    692\u001b[39m             probs_at_sot = logits[:, \u001b[38;5;28mself\u001b[39m.sot_index].float().softmax(dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/decoding.py:163\u001b[39m, in \u001b[36mPyTorchInference.logits\u001b[39m\u001b[34m(self, tokens, audio_features)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokens.shape[-\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.initial_token_length:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[32m    161\u001b[39m     tokens = tokens[:, -\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/model.py:242\u001b[39m, in \u001b[36mTextDecoder.forward\u001b[39m\u001b[34m(self, x, xa, kv_cache)\u001b[39m\n\u001b[32m    239\u001b[39m x = x.to(xa.dtype)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    245\u001b[39m logits = (\n\u001b[32m    246\u001b[39m     x @ torch.transpose(\u001b[38;5;28mself\u001b[39m.token_embedding.weight.to(x.dtype), \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    247\u001b[39m ).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/model.py:167\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    161\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    162\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m     kv_cache: Optional[\u001b[38;5;28mdict\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    166\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m    169\u001b[39m         x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28mself\u001b[39m.cross_attn_ln(x), xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/model.py:111\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    108\u001b[39m     k = kv_cache[\u001b[38;5;28mself\u001b[39m.key]\n\u001b[32m    109\u001b[39m     v = kv_cache[\u001b[38;5;28mself\u001b[39m.value]\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m wv, qk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqkv_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out(wv), qk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LBMP/.venv/lib/python3.11/site-packages/whisper/model.py:124\u001b[39m, in \u001b[36mMultiHeadAttention.qkv_attention\u001b[39m\u001b[34m(self, q, k, v, mask)\u001b[39m\n\u001b[32m    121\u001b[39m v = v.view(*v.shape[:\u001b[32m2\u001b[39m], \u001b[38;5;28mself\u001b[39m.n_head, -\u001b[32m1\u001b[39m).permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SDPA_AVAILABLE \u001b[38;5;129;01mand\u001b[39;00m MultiHeadAttention.use_sdpa:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     a = \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     out = a.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).flatten(start_dim=\u001b[32m2\u001b[39m)\n\u001b[32m    128\u001b[39m     qk = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "whisper_model = load_whisper_models()\n",
    "print(\"Whisper models loaded.\")\n",
    "\n",
    "api_key = os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCAQ7iYFHHlLi9pqKku_j_elEp9hOVO5Ng\" \n",
    "\n",
    "llm = setup_google_llm(api_key, model_name=\"gemini-2.0-flash\", temperature=0.7)\n",
    "print(\"LLM loaded.\")\n",
    "    \n",
    "# Loading the external drive\n",
    "data_dir = \"/Volumes/My Passport for Mac/Extended DAIC-WOZ Database/data\"\n",
    "    \n",
    "# Selecting the partcicipants to process\n",
    "train_csv_path = \"/Volumes/My Passport for Mac/Extended DAIC-WOZ Database/labels/train_split.csv\"\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Selecting only WoZ participants\n",
    "filtered_train_df = train_df[train_df['Participant_ID'].between(300, 492)]\n",
    "\n",
    "# Selecting 50 random patients for the finetuning subset\n",
    "train_ids = list(filtered_train_df['Participant_ID'].astype(str))\n",
    "random_50_patients = random.sample(train_ids, 50)\n",
    "print(\"Random 50 patients selected:\", random_50_patients)\n",
    "    \n",
    "# Run the processing to create new transctripts and summaries for eachh patient\n",
    "transcripts_df = extract_and_process_patient_archives(\n",
    "    data_dir, train_csv_path, llm, whisper_model, patients_to_process=random_50_patients\n",
    ")\n",
    "    \n",
    "transcripts_df.to_csv(\"train_patient_transcripts_50_cleaned.tsv\", sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Participant_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PHQ_Binary",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PHQ_Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PCL-C (PTSD)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PTSD Severity",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "6307930e-2a5f-4bc8-9819-71fe39ab459f",
       "rows": [
        [
         "0",
         "302",
         "male",
         "0",
         "4",
         "0",
         "28"
        ],
        [
         "1",
         "303",
         "female",
         "0",
         "0",
         "0",
         "17"
        ],
        [
         "2",
         "304",
         "female",
         "0",
         "6",
         "0",
         "20"
        ],
        [
         "3",
         "305",
         "male",
         "0",
         "7",
         "0",
         "28"
        ],
        [
         "4",
         "307",
         "female ",
         "0",
         "4",
         "0",
         "23"
        ],
        [
         "5",
         "308",
         "female ",
         "1",
         "22",
         "1",
         "67"
        ],
        [
         "6",
         "309",
         "male ",
         "1",
         "15",
         "1",
         "74"
        ],
        [
         "7",
         "310",
         "male",
         "0",
         "4",
         "0",
         "35"
        ],
        [
         "8",
         "311",
         "female",
         "1",
         "21",
         "1",
         "70"
        ],
        [
         "9",
         "312",
         "male",
         "0",
         "2",
         "0",
         "18"
        ],
        [
         "10",
         "313",
         "male",
         "0",
         "7",
         "0",
         "44"
        ],
        [
         "11",
         "314",
         "female",
         "0",
         "1",
         "0",
         "19"
        ],
        [
         "12",
         "315",
         "male ",
         "0",
         "2",
         "0",
         "18"
        ],
        [
         "13",
         "316",
         "male",
         "0",
         "6",
         "0",
         "33"
        ],
        [
         "14",
         "318",
         "male",
         "0",
         "3",
         "0",
         "19"
        ],
        [
         "15",
         "319",
         "male",
         "1",
         "13",
         "1",
         "53"
        ],
        [
         "16",
         "322",
         "male",
         "0",
         "5",
         "0",
         "23"
        ],
        [
         "17",
         "323",
         "female",
         "0",
         "1",
         "0",
         "19"
        ],
        [
         "18",
         "324",
         "male",
         "0",
         "5",
         "0",
         "50"
        ],
        [
         "19",
         "325",
         "female",
         "0",
         "10",
         "0",
         "30"
        ],
        [
         "20",
         "326",
         "male",
         "0",
         "2",
         "0",
         "28"
        ],
        [
         "21",
         "327",
         "female",
         "0",
         "4",
         "0",
         "25"
        ],
        [
         "22",
         "328",
         "male",
         "0",
         "4",
         "0",
         "18"
        ],
        [
         "23",
         "329",
         "male",
         "0",
         "1",
         "0",
         "17"
        ],
        [
         "24",
         "330",
         "male",
         "1",
         "12",
         "1",
         "46"
        ],
        [
         "25",
         "332",
         "female",
         "1",
         "18",
         "1",
         "58"
        ],
        [
         "26",
         "333",
         "male",
         "0",
         "5",
         "0",
         "23"
        ],
        [
         "27",
         "335",
         "female",
         "0",
         "12",
         "0",
         "33"
        ],
        [
         "28",
         "337",
         "female",
         "1",
         "10",
         "1",
         "51"
        ],
        [
         "29",
         "338",
         "female",
         "1",
         "15",
         "1",
         "51"
        ],
        [
         "30",
         "339",
         "male",
         "1",
         "11",
         "0",
         "42"
        ],
        [
         "31",
         "340",
         "male",
         "0",
         "1",
         "0",
         "22"
        ],
        [
         "32",
         "341",
         "female",
         "0",
         "7",
         "0",
         "30"
        ],
        [
         "33",
         "345",
         "female",
         "1",
         "15",
         "1",
         "67"
        ],
        [
         "34",
         "346",
         "female",
         "1",
         "23",
         "1",
         "85"
        ],
        [
         "35",
         "348",
         "female",
         "1",
         "20",
         "1",
         "67"
        ],
        [
         "36",
         "349",
         "male",
         "0",
         "5",
         "0",
         "36"
        ],
        [
         "37",
         "351",
         "female",
         "1",
         "14",
         "1",
         "46"
        ],
        [
         "38",
         "352",
         "female",
         "0",
         "10",
         "0",
         "35"
        ],
        [
         "39",
         "353",
         "female",
         "1",
         "11",
         "1",
         "55"
        ],
        [
         "40",
         "354",
         "male",
         "1",
         "18",
         "1",
         "59"
        ],
        [
         "41",
         "355",
         "male",
         "1",
         "10",
         "1",
         "45"
        ],
        [
         "42",
         "356",
         "male",
         "0",
         "10",
         "1",
         "48"
        ],
        [
         "43",
         "357",
         "male",
         "0",
         "7",
         "0",
         "37"
        ],
        [
         "44",
         "358",
         "male",
         "0",
         "7",
         "0",
         "50"
        ],
        [
         "45",
         "359",
         "male",
         "1",
         "13",
         "1",
         "39"
        ],
        [
         "46",
         "360",
         "female",
         "0",
         "4",
         "0",
         "26"
        ],
        [
         "47",
         "361",
         "male",
         "0",
         "0",
         "0",
         "17"
        ],
        [
         "48",
         "362",
         "female",
         "1",
         "20",
         "1",
         "71"
        ],
        [
         "49",
         "363",
         "male",
         "0",
         "0",
         "0",
         "17"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 163
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ_Binary</th>\n",
       "      <th>PHQ_Score</th>\n",
       "      <th>PCL-C (PTSD)</th>\n",
       "      <th>PTSD Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>303</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>305</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>307</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>695</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>697</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>702</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>703</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>707</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant_ID   Gender  PHQ_Binary  PHQ_Score  PCL-C (PTSD)  \\\n",
       "0               302     male           0          4             0   \n",
       "1               303   female           0          0             0   \n",
       "2               304   female           0          6             0   \n",
       "3               305     male           0          7             0   \n",
       "4               307  female            0          4             0   \n",
       "..              ...      ...         ...        ...           ...   \n",
       "158             695     male           0          7             1   \n",
       "159             697     male           0          5             0   \n",
       "160             702     male           0          0             0   \n",
       "161             703     male           0          8             0   \n",
       "162             707     male           0          1             0   \n",
       "\n",
       "     PTSD Severity  \n",
       "0               28  \n",
       "1               17  \n",
       "2               20  \n",
       "3               28  \n",
       "4               23  \n",
       "..             ...  \n",
       "158             62  \n",
       "159             24  \n",
       "160             19  \n",
       "161             28  \n",
       "162             23  \n",
       "\n",
       "[163 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/Users/joaomata/Desktop/LBMP/E-DAIC/labels/train_split.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Participant_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PHQ_Binary",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PHQ_Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PCL-C (PTSD)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PTSD Severity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dep_Severity",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "254ed748-1b15-4705-9297-4dc95abbea0e",
       "rows": [
        [
         "0",
         "302",
         "male",
         "0",
         "4",
         "0",
         "28",
         "not depression"
        ],
        [
         "1",
         "303",
         "female",
         "0",
         "0",
         "0",
         "17",
         "not depression"
        ],
        [
         "2",
         "304",
         "female",
         "0",
         "6",
         "0",
         "20",
         "not depression"
        ],
        [
         "3",
         "305",
         "male",
         "0",
         "7",
         "0",
         "28",
         "not depression"
        ],
        [
         "4",
         "307",
         "female ",
         "0",
         "4",
         "0",
         "23",
         "not depression"
        ],
        [
         "5",
         "308",
         "female ",
         "1",
         "22",
         "1",
         "67",
         "severe"
        ],
        [
         "6",
         "309",
         "male ",
         "1",
         "15",
         "1",
         "74",
         "severe"
        ],
        [
         "7",
         "310",
         "male",
         "0",
         "4",
         "0",
         "35",
         "not depression"
        ],
        [
         "8",
         "311",
         "female",
         "1",
         "21",
         "1",
         "70",
         "severe"
        ],
        [
         "9",
         "312",
         "male",
         "0",
         "2",
         "0",
         "18",
         "not depression"
        ],
        [
         "10",
         "313",
         "male",
         "0",
         "7",
         "0",
         "44",
         "not depression"
        ],
        [
         "11",
         "314",
         "female",
         "0",
         "1",
         "0",
         "19",
         "not depression"
        ],
        [
         "12",
         "315",
         "male ",
         "0",
         "2",
         "0",
         "18",
         "not depression"
        ],
        [
         "13",
         "316",
         "male",
         "0",
         "6",
         "0",
         "33",
         "not depression"
        ],
        [
         "14",
         "318",
         "male",
         "0",
         "3",
         "0",
         "19",
         "not depression"
        ],
        [
         "15",
         "319",
         "male",
         "1",
         "13",
         "1",
         "53",
         "moderate"
        ],
        [
         "16",
         "322",
         "male",
         "0",
         "5",
         "0",
         "23",
         "not depression"
        ],
        [
         "17",
         "323",
         "female",
         "0",
         "1",
         "0",
         "19",
         "not depression"
        ],
        [
         "18",
         "324",
         "male",
         "0",
         "5",
         "0",
         "50",
         "not depression"
        ],
        [
         "19",
         "325",
         "female",
         "0",
         "10",
         "0",
         "30",
         "moderate"
        ],
        [
         "20",
         "326",
         "male",
         "0",
         "2",
         "0",
         "28",
         "not depression"
        ],
        [
         "21",
         "327",
         "female",
         "0",
         "4",
         "0",
         "25",
         "not depression"
        ],
        [
         "22",
         "328",
         "male",
         "0",
         "4",
         "0",
         "18",
         "not depression"
        ],
        [
         "23",
         "329",
         "male",
         "0",
         "1",
         "0",
         "17",
         "not depression"
        ],
        [
         "24",
         "330",
         "male",
         "1",
         "12",
         "1",
         "46",
         "moderate"
        ],
        [
         "25",
         "332",
         "female",
         "1",
         "18",
         "1",
         "58",
         "severe"
        ],
        [
         "26",
         "333",
         "male",
         "0",
         "5",
         "0",
         "23",
         "not depression"
        ],
        [
         "27",
         "335",
         "female",
         "0",
         "12",
         "0",
         "33",
         "moderate"
        ],
        [
         "28",
         "337",
         "female",
         "1",
         "10",
         "1",
         "51",
         "moderate"
        ],
        [
         "29",
         "338",
         "female",
         "1",
         "15",
         "1",
         "51",
         "severe"
        ],
        [
         "30",
         "339",
         "male",
         "1",
         "11",
         "0",
         "42",
         "moderate"
        ],
        [
         "31",
         "340",
         "male",
         "0",
         "1",
         "0",
         "22",
         "not depression"
        ],
        [
         "32",
         "341",
         "female",
         "0",
         "7",
         "0",
         "30",
         "not depression"
        ],
        [
         "33",
         "345",
         "female",
         "1",
         "15",
         "1",
         "67",
         "severe"
        ],
        [
         "34",
         "346",
         "female",
         "1",
         "23",
         "1",
         "85",
         "severe"
        ],
        [
         "35",
         "348",
         "female",
         "1",
         "20",
         "1",
         "67",
         "severe"
        ],
        [
         "36",
         "349",
         "male",
         "0",
         "5",
         "0",
         "36",
         "not depression"
        ],
        [
         "37",
         "351",
         "female",
         "1",
         "14",
         "1",
         "46",
         "severe"
        ],
        [
         "38",
         "352",
         "female",
         "0",
         "10",
         "0",
         "35",
         "moderate"
        ],
        [
         "39",
         "353",
         "female",
         "1",
         "11",
         "1",
         "55",
         "moderate"
        ],
        [
         "40",
         "354",
         "male",
         "1",
         "18",
         "1",
         "59",
         "severe"
        ],
        [
         "41",
         "355",
         "male",
         "1",
         "10",
         "1",
         "45",
         "moderate"
        ],
        [
         "42",
         "356",
         "male",
         "0",
         "10",
         "1",
         "48",
         "moderate"
        ],
        [
         "43",
         "357",
         "male",
         "0",
         "7",
         "0",
         "37",
         "not depression"
        ],
        [
         "44",
         "358",
         "male",
         "0",
         "7",
         "0",
         "50",
         "not depression"
        ],
        [
         "45",
         "359",
         "male",
         "1",
         "13",
         "1",
         "39",
         "moderate"
        ],
        [
         "46",
         "360",
         "female",
         "0",
         "4",
         "0",
         "26",
         "not depression"
        ],
        [
         "47",
         "361",
         "male",
         "0",
         "0",
         "0",
         "17",
         "not depression"
        ],
        [
         "48",
         "362",
         "female",
         "1",
         "20",
         "1",
         "71",
         "severe"
        ],
        [
         "49",
         "363",
         "male",
         "0",
         "0",
         "0",
         "17",
         "not depression"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 163
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ_Binary</th>\n",
       "      <th>PHQ_Score</th>\n",
       "      <th>PCL-C (PTSD)</th>\n",
       "      <th>PTSD Severity</th>\n",
       "      <th>Dep_Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>303</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>305</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>307</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>695</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>697</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>702</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>703</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>707</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant_ID   Gender  PHQ_Binary  PHQ_Score  PCL-C (PTSD)  \\\n",
       "0               302     male           0          4             0   \n",
       "1               303   female           0          0             0   \n",
       "2               304   female           0          6             0   \n",
       "3               305     male           0          7             0   \n",
       "4               307  female            0          4             0   \n",
       "..              ...      ...         ...        ...           ...   \n",
       "158             695     male           0          7             1   \n",
       "159             697     male           0          5             0   \n",
       "160             702     male           0          0             0   \n",
       "161             703     male           0          8             0   \n",
       "162             707     male           0          1             0   \n",
       "\n",
       "     PTSD Severity    Dep_Severity  \n",
       "0               28  not depression  \n",
       "1               17  not depression  \n",
       "2               20  not depression  \n",
       "3               28  not depression  \n",
       "4               23  not depression  \n",
       "..             ...             ...  \n",
       "158             62  not depression  \n",
       "159             24  not depression  \n",
       "160             19  not depression  \n",
       "161             28        moderate  \n",
       "162             23  not depression  \n",
       "\n",
       "[163 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Dep_Severity'] = train_df['PHQ_Score'].apply(lambda x: 'not depression' if x <= 7 else ('moderate' if x <= 13 else 'severe'))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
